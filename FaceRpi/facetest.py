import os
import cv2
import face_recognition
import threading
import requests
import time
import tempfile
import pickle
from collections import deque
import paho.mqtt.client as mqtt
import ssl

# ---------- C·∫§U H√åNH (ƒêi·ªÅn th√¥ng tin c·ªßa b·∫°n v√†o ƒë√¢y) ----------
# ƒê·∫£m b·∫£o dataset n·∫±m k·∫ø b√™n file facetest.py
SCRIPT_DIR     = os.path.dirname(os.path.abspath(__file__))
DATASET_DIR    = os.path.join(SCRIPT_DIR, "dataset")
TELEGRAM_TOKEN   = "YOUR_TELEGRAM_TOKEN"
TELEGRAM_CHAT_ID = "YOUR_TELEGRAM_CHAT_ID"
MIN_CONFIDENCE   = 0.45
VOTE_HISTORY     = 5

# MQTT broker (hostname ho·∫∑c IP)
mqtt_broker = "28766de7c6c947dd865b4f3ab34e8883.s1.eu.hivemq.cloud"
mqtt_port   = 8883
mqtt_user   = "haichu"
mqtt_pass   = "H@ichu321"
mqtt_topic  = "home/control"

# T·∫°o th∆∞ m·ª•c dataset n·∫øu ch∆∞a c√≥
os.makedirs(DATASET_DIR, exist_ok=True)

# Globals
known_face_encodings = []
known_face_names     = []
frame_to_check       = None
name                 = "Scanning..."
result_lock          = threading.Lock()
vote_buffer          = deque(maxlen=VOTE_HISTORY)


def send_telegram_alert(message, image_path=None):
    """G·ª≠i alert qua Telegram"""
    try:
        base = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}"
        requests.post(f"{base}/sendMessage",
                      data={"chat_id": TELEGRAM_CHAT_ID, "text": message},
                      timeout=5)
        if image_path:
            with open(image_path, 'rb') as photo:
                requests.post(f"{base}/sendPhoto",
                              data={"chat_id": TELEGRAM_CHAT_ID},
                              files={"photo": photo},
                              timeout=5)
    except Exception as e:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ g·ª≠i Telegram:", e)


def send_mqtt_message(client, message):
    """G·ª≠i message qua MQTT"""
    try:
        client.publish(mqtt_topic, message)
        print(f"[MQTT] ƒê√£ g·ª≠i: {message} -> {mqtt_topic}")
    except Exception as e:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ g·ª≠i MQTT:", e)


def apply_clahe(image):
    """C·∫£i thi·ªán t∆∞∆°ng ph·∫£n b·∫±ng CLAHE"""
    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    cl = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(l)
    merged = cv2.merge((cl, a, b))
    return cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)


def load_known_faces():
    """Load cache ho·∫∑c encode m·ªõi b·ªô dataset"""
    global known_face_encodings, known_face_names
    cache_path = os.path.join(DATASET_DIR, "encodings.pkl")
    if os.path.exists(cache_path):
        with open(cache_path, "rb") as f:
            known_face_encodings, known_face_names = pickle.load(f)
        print(f"‚úÖ ƒê√£ t·∫£i {len(known_face_names)} ng∆∞·ªùi t·ª´ cache.")
        return

    known_face_encodings = []
    known_face_names     = []
    for file in os.listdir(DATASET_DIR):
        if file.lower().endswith((".jpg", ".png")):
            path = os.path.join(DATASET_DIR, file)
            image = face_recognition.load_image_file(path)
            encs = face_recognition.face_encodings(image)
            if encs:
                known_face_encodings.append(encs[0])
                person = os.path.splitext(file)[0].split("_")[0]
                known_face_names.append(person)

    with open(cache_path, "wb") as f:
        pickle.dump((known_face_encodings, known_face_names), f)
    print(f"‚úÖ ƒê√£ encode v√† l∆∞u cache {len(known_face_names)} ng∆∞·ªùi.")


def add_face_from_webcam():
    """Th√™m ng∆∞·ªùi m·ªõi qua webcam USB v·ªõi preview ch·∫•t l∆∞·ª£ng cao"""
    person_name = input("Nh·∫≠p t√™n ng∆∞·ªùi m·ªõi: ").strip()
    cam_id = int(input("Nh·∫≠p ID camera (m·∫∑c ƒë·ªãnh 0): ") or 0)
    cap = cv2.VideoCapture(cam_id)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

    count = 0
    max_images = 5
    print("üì∏ Nh·∫•n 's' ƒë·ªÉ ch·ª•p, 'q' ƒë·ªÉ tho√°t.")
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        disp = cv2.resize(frame, (960, 540), interpolation=cv2.INTER_LINEAR)
        cv2.imshow("Th√™m khu√¥n m·∫∑t", disp)
        key = cv2.waitKey(1) & 0xFF

        if key == ord('s') and count < max_images:
            locs = face_recognition.face_locations(frame, model='hog')
            print("Detected boxes:", locs)
            if locs:
                top, right, bottom, left = locs[0]
                face_img = frame[top:bottom, left:right]
                filename = os.path.join(DATASET_DIR, f"{person_name}_{count+1}.jpg")
                cv2.imwrite(filename, face_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])
                print(f"‚úÖ ƒê√£ l∆∞u: {filename}")
                count += 1
            else:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t, th·ª≠ l·∫°i.")
        elif key == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    cache_path = os.path.join(DATASET_DIR, "encodings.pkl")
    if os.path.exists(cache_path):
        os.remove(cache_path)
    load_known_faces()


def recognize_background(client):
    """Thread nh·∫≠n di·ªán n·ªÅn, g·ª≠i alert khi c√≥ thay ƒë·ªïi"""
    global name, frame_to_check, vote_buffer
    previous_name = ""

    while True:
        if frame_to_check is None:
            time.sleep(0.01)
            continue

        with result_lock:
            frame = frame_to_check.copy()
            frame_to_check = None

        small = apply_clahe(frame)
        rgb   = cv2.cvtColor(small, cv2.COLOR_BGR2RGB)
        locs  = face_recognition.face_locations(rgb, model='hog')
        encs  = face_recognition.face_encodings(rgb, locs)

        recognized = "Not authorized"
        for e in encs:
            dists = face_recognition.face_distance(known_face_encodings, e)
            idx   = dists.argmin() if dists.size else None
            if idx is not None and dists[idx] < MIN_CONFIDENCE:
                recognized = known_face_names[idx]

        vote_buffer.append(recognized)
        voted = max(set(vote_buffer), key=vote_buffer.count)

        with result_lock:
            name = voted

        if voted != previous_name:
            if voted == "Not authorized":
                print("üîí Ph√°t hi·ªán ng∆∞·ªùi l·∫°!")
                imgfile = tempfile.mktemp(suffix=".jpg")
                cv2.imwrite(imgfile, frame)
                send_telegram_alert("üö® C·∫¢NH B√ÅO: Ng∆∞·ªùi l·∫° tr∆∞·ªõc camera!", imgfile)
                send_mqtt_message(client, "Ng∆∞·ªùi l·∫° tr∆∞·ªõc camera!")
            else:
                print(f"‚úÖ Nh·∫≠n di·ªán: {voted}")
                send_mqtt_message(client, f"Nh·∫≠n di·ªán: {voted}")
            previous_name = voted


def start_recognition(client):
    """Ch·∫°y nh·∫≠n di·ªán li√™n t·ª•c, hi·ªÉn th·ªã khung"""
    global frame_to_check, name
    threading.Thread(target=recognize_background, args=(client,), daemon=True).start()

    cam_id = int(input("Nh·∫≠p ID camera (m·∫∑c ƒë·ªãnh 0): ") or 0)
    cap = cv2.VideoCapture(cam_id)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540)

    print("üîç ƒêang ch·∫°y nh·∫≠n di·ªán... Nh·∫•n 'q' ƒë·ªÉ tho√°t.")
    frame_count = 0
    interval    = 10

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        with result_lock:
            display_name = name

        disp = frame.copy()
        gray = cv2.cvtColor(disp, cv2.COLOR_BGR2GRAY)
        cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        faces = cascade.detectMultiScale(gray, 1.3, 5)

        for (x, y, w, h) in faces:
            if w < 80 or h < 80:
                continue
            color = (0,255,0) if display_name != "Not authorized" else (0,0,255)
            cv2.rectangle(disp, (x,y), (x+w,y+h), color, 2)
            cv2.putText(disp, display_name, (x, y-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

            if frame_count % interval == 0 and frame_to_check is None:
                with result_lock:
                    face_img = frame[y:y+h, x:x+w]
                    frame_to_check = cv2.resize(face_img, (150,150))

        cv2.imshow("Face Recognition", disp)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

        frame_count += 1
        time.sleep(0.01)

    cap.release()
    cv2.destroyAllWindows()


def create_mqtt_client():
    """Kh·ªüi t·∫°o MQTT client, d√πng MQTTv311 ƒë·ªÉ tr√°nh c·∫£nh b√°o"""
    client = mqtt.Client(protocol=mqtt.MQTTv311)
    client.username_pw_set(mqtt_user, mqtt_pass)
    client.tls_set(cert_reqs=ssl.CERT_REQUIRED)
    client.connect(mqtt_broker, mqtt_port, keepalive=60)
    return client


def main_menu():
    client = create_mqtt_client()
    load_known_faces()
    while True:
        print("\n===== MENU =====")
        print("1. Nh·∫≠n di·ªán khu√¥n m·∫∑t")
        print("2. Th√™m ng∆∞·ªùi m·ªõi")
        print("0. Tho√°t")
        choice = input("Ch·ªçn: ").strip()
        if choice == "1":
            start_recognition(client)
        elif choice == "2":
            add_face_from_webcam()
        elif choice == "0":
            client.disconnect()
            print("üëã T·∫°m bi·ªát!")
            break
        else:
            print("‚ùå L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")


if __name__ == "__main__":
    main_menu()
